# app_langgraph.py - ì±•í„° 3: LangGraphë¥¼ í™œìš©í•œ AI í† ë¡  ì‹œìŠ¤í…œ êµ¬í˜„
from enum import Enum, auto
import streamlit as st
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import HumanMessage, SystemMessage, AIMessage, Document
from langgraph.graph import StateGraph, END
from typing import Dict, List, TypedDict, Literal
import os
from dotenv import load_dotenv
import wikipedia
import sqlite3
import json
from datetime import datetime

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="AI í† ë¡ ", page_icon="ğŸ¤–", layout="wide")

# ì œëª© ë° ì†Œê°œ
st.title("ğŸ¤– AI í† ë¡  - LangGraph & RAG ë²„ì „")
st.markdown(
    """
    ### í”„ë¡œì íŠ¸ ì†Œê°œ
    ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ LangGraphë¥¼ í™œìš©í•˜ì—¬ AI ì—ì´ì „íŠ¸ ê°„ì˜ í† ë¡  ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
    ì°¬ì„± ì¸¡, ë°˜ëŒ€ ì¸¡, ê·¸ë¦¬ê³  ì‹¬íŒ ì—­í• ì˜ AIê°€ ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ ì²´ê³„ì ìœ¼ë¡œ í† ë¡ ì„ ì§„í–‰í•©ë‹ˆë‹¤.
    RAG(Retrieval-Augmented Generation)ë¥¼ í†µí•´ ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ ë” ê°•ë ¥í•œ ë…¼ë¦¬ë¥¼ í¼ì¹©ë‹ˆë‹¤.
    """
)

# LLM ì„¤ì •
llm = AzureChatOpenAI(
    openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT"),
    api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2023-05-15"),
    temperature=0.7,
)

# ì„ë² ë”© ì„¤ì •
embeddings = AzureOpenAIEmbeddings(
    model=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT"),
    openai_api_version="2024-02-01",
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
)


# SQLite DB ì´ˆê¸°í™”
def init_db():
    """DB íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³  í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì´ˆê¸°í™”"""
    import os.path
    
    db_exists = os.path.exists("debate_history.db")
    
    if not db_exists:
        # DBê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì´ˆê¸°í™”
        print("Initializing new database...")
        conn = sqlite3.connect("debate_history.db")
        c = conn.cursor()
        c.execute(
            """
            CREATE TABLE IF NOT EXISTS debates (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                topic TEXT NOT NULL,
                date TEXT NOT NULL,
                rounds INTEGER NOT NULL,
                messages TEXT NOT NULL,
                retrieved_docs TEXT
            )
        """
        )
        conn.commit()
        conn.close()
    # else:
    #     print("Database already exists, skipping initialization.")

# DB load
def load_debate_history():
    """DBì—ì„œ í† ë¡  ì´ë ¥ì„ ë¡œë“œ"""
    conn = sqlite3.connect("debate_history.db")
    c = conn.cursor()
    c.execute("SELECT topic, date, rounds, messages FROM debates ORDER BY date DESC")
    debates = c.fetchall()
    conn.close()
    
    # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ìš© í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    debate_history = []
    for debate in debates:
        debate_history.append({
            "topic": debate[0],
            "timestamp": debate[1],
            "rounds": debate[2],
            "messages": json.loads(debate[3])  # JSON ë¬¸ìì—´ì„ íŒŒì´ì¬ ê°ì²´ë¡œ ë³€í™˜
        })
    
    return debate_history


# ê²€ìƒ‰ ì§ˆì˜ì–´ ê°œì„  í•¨ìˆ˜
def improve_search_query(topic, search_type="general", language="en"):
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ì§ˆì˜ì–´ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.

    Args:
        topic: ì›ë˜ í† ë¡  ì£¼ì œ
        search_type: "pro" (ì°¬ì„±), "con" (ë°˜ëŒ€), "general" (ì¼ë°˜) ì¤‘ í•˜ë‚˜
        language: ê²€ìƒ‰ ì–¸ì–´ ("en" ë˜ëŠ” "ko")

    Returns:
        ê°œì„ ëœ ê²€ìƒ‰ ì§ˆì˜ì–´ ëª©ë¡
    """
    prompt_by_type = {
        "pro": f"'{topic}'ì— ëŒ€í•´ ì°¬ì„±í•˜ëŠ” ì…ì¥ì„ ë’·ë°›ì¹¨í•  ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ê³¼ ì •ë³´ë¥¼ ì°¾ê³ ì í•©ë‹ˆë‹¤. ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰ì— ì í•©í•œ 3ê°œì˜ ê²€ìƒ‰ì–´ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. ê° ê²€ìƒ‰ì–´ëŠ” 25ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì„¸ìš”. ê²€ìƒ‰ì–´ë§Œ ì œê³µí•˜ê³  ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.",
        "con": f"'{topic}'ì— ëŒ€í•´ ë°˜ëŒ€í•˜ëŠ” ì…ì¥ì„ ë’·ë°›ì¹¨í•  ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ê³¼ ì •ë³´ë¥¼ ì°¾ê³ ì í•©ë‹ˆë‹¤. ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰ì— ì í•©í•œ 3ê°œì˜ ê²€ìƒ‰ì–´ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. ê° ê²€ìƒ‰ì–´ëŠ” 25ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì„¸ìš”. ê²€ìƒ‰ì–´ë§Œ ì œê³µí•˜ê³  ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.",
        "general": f"'{topic}'ì— ëŒ€í•œ ê°ê´€ì ì¸ ì‚¬ì‹¤ê³¼ ì •ë³´ë¥¼ ì°¾ê³ ì í•©ë‹ˆë‹¤. ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰ì— ì í•©í•œ 3ê°œì˜ ê²€ìƒ‰ì–´ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”. ê° ê²€ìƒ‰ì–´ëŠ” 25ì ì´ë‚´ë¡œ ì‘ì„±í•˜ê³  ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì„¸ìš”. ê²€ìƒ‰ì–´ë§Œ ì œê³µí•˜ê³  ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.",
    }

    messages = [
        SystemMessage(
            content="ë‹¹ì‹ ì€ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²€ìƒ‰ì–´ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”."
        ),
        HumanMessage(content=prompt_by_type[search_type]),
    ]

    try:
        # ë‚®ì€ ì˜¨ë„ë¡œ ì„¤ì •í•˜ì—¬ ì¼ê´€ëœ ê²°ê³¼ ìœ ë„
        response = llm.invoke(messages)
        # ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ê²€ìƒ‰ì–´ ë¶„ë¦¬
        suggested_queries = [q.strip() for q in response.content.split(",")]
        # ìµœëŒ€ 3ê°œ ê²€ìƒ‰ì–´ë§Œ ì‚¬ìš©
        return suggested_queries[:3]
    except Exception as e:
        st.warning(f"ê²€ìƒ‰ì–´ ê°œì„  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        # ê¸°ë³¸ ê²€ìƒ‰ì–´ ë°˜í™˜
        if search_type == "pro":
            return [f"{topic} advantages", f"{topic} benefits", f"{topic} support"]
        elif search_type == "con":
            return [f"{topic} disadvantages", f"{topic} problems", f"{topic} against"]
        else:
            return [topic]


# ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
def get_wikipedia_content(topic, language="en", search_type="general"):
    st.divider()
    try:
        # ì–¸ì–´ ì„¤ì •
        wikipedia.set_lang(language)

        # LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì–´ ê°œì„ 
        improved_queries = improve_search_query(topic, search_type, language)
        st.info(f"ê°œì„ ëœ ê²€ìƒ‰ì–´: {', '.join(improved_queries)}")

        documents = []

        # ê° ê°œì„ ëœ ê²€ìƒ‰ì–´ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
        for query in improved_queries:
            # ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            search_results = wikipedia.search(query, results=3)

            if not search_results:
                continue

            # ìµœëŒ€ 2ê°œì˜ ê´€ë ¨ì„± ë†’ì€ í˜ì´ì§€ë§Œ ì²˜ë¦¬ (ì¿¼ë¦¬ë‹¹)
            for page_title in search_results[:2]:
                try:
                    # í˜ì´ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    page = wikipedia.page(page_title, auto_suggest=False)

                    # ì´ë¯¸ ìˆ˜ì§‘í•œ í˜ì´ì§€ì¸ì§€ í™•ì¸
                    if any(
                        doc.metadata.get("topic") == page_title for doc in documents
                    ):
                        continue

                    # ìš”ì•½ ì¶”ê°€
                    if page.summary:
                        documents.append(
                            Document(
                                page_content=page.summary,
                                metadata={
                                    "source": f"wikipedia-{language}",
                                    "section": "summary",
                                    "topic": page_title,
                                    "query": query,
                                },
                            )
                        )

                    # ë³¸ë¬¸ ì„¹ì…˜ ì¶”ê°€ (ì¼ë¶€ë§Œ)
                    content = page.content
                    if content:
                        # ë³¸ë¬¸ì„ ì ë‹¹í•œ ê¸¸ì´ë¡œ ìë¥´ê¸°
                        max_length = 5000
                        if len(content) > max_length:
                            content = content[:max_length]

                        documents.append(
                            Document(
                                page_content=content,
                                metadata={
                                    "source": f"wikipedia-{language}",
                                    "section": "content",
                                    "topic": page_title,
                                    "query": query,
                                },
                            )
                        )
                except (
                    wikipedia.exceptions.DisambiguationError,
                    wikipedia.exceptions.PageError,
                ) as e:
                    continue

        if documents:
            st.success(f"{language} ì–¸ì–´ë¡œ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

        return documents

    except Exception as e:
        st.error(f"ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []


# Vector Store ìƒì„± í•¨ìˆ˜
@st.cache_resource
def create_vector_store(topic):
    documents = []

    # ì˜ì–´ ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ìˆ˜ì§‘
    wiki_docs_en = get_wikipedia_content(topic, "en")
    if wiki_docs_en:
        documents.extend(wiki_docs_en)

    # í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„ ë°ì´í„° ìˆ˜ì§‘
    wiki_docs_ko = get_wikipedia_content(topic, "ko")
    if wiki_docs_ko:
        documents.extend(wiki_docs_ko)

    # í† í”½ì´ ì˜ì–´ê°€ ì•„ë‹Œ ê²½ìš° ì˜ì–´ë¡œë„ ê²€ìƒ‰ ì‹œë„
    if not any(c.isascii() for c in topic):
        # í•œêµ­ì–´ ì£¼ì œë¥¼ ì˜ì–´ë¡œ ë³€í™˜í•˜ë ¤ëŠ” ì‹œë„
        try:
            english_topic = f"{topic} in English"
            additional_docs = get_wikipedia_content(english_topic, "en")
            if additional_docs:
                documents.extend(additional_docs)
        except:
            pass

    # Vector DB ìƒì„±
    if documents:
        try:
            vector_store = FAISS.from_documents(documents, embeddings)
            return vector_store
        except Exception as e:
            st.error(f"Vector DB ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
    else:
        return None


# ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
def retrieve_relevant_info(query, vector_store, k=3):
    if not vector_store:
        return "", []

    try:
        # ì§ˆì˜ì— ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = vector_store.similarity_search(query, k=k)

        # ê²€ìƒ‰ ê²°ê³¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        context = ""
        for i, doc in enumerate(retrieved_docs):
            source = doc.metadata.get("source", "Unknown")
            section = doc.metadata.get("section", "")
            context += f"[ë¬¸ì„œ {i+1}] ì¶œì²˜: {source}"
            if section:
                context += f", ì„¹ì…˜: {section}"
            context += f"\n{doc.page_content}\n\n"

        return context, retrieved_docs
    except:
        return "", []


# í† ë¡ ì ì—­í• ì„ ìœ„í•œ Enum
class SpeakerRole(Enum):
    PRO = "pro_agent"
    CON = "con_agent"
    JUDGE = "judge"
    COMPLETED = "completed"


# í† ë¡  ìƒíƒœë¥¼ ìœ„í•œ Enum
class DebateStatus(Enum):
    ACTIVE = auto()
    JUDGED = auto()
    COMPLETED = auto()


# LangGraph ìƒíƒœ ì •ì˜ - RAG ê´€ë ¨ í•„ë“œ ì¶”ê°€ ë° ëª…í™•í™”
class DebateState(TypedDict):
    topic: str
    messages: List[Dict]
    current_round: int
    max_rounds: int
    current_speaker: SpeakerRole
    debate_status: DebateStatus
    vector_store: object  # RAG ë²¡í„° ìŠ¤í† ì–´
    retrieved_docs: Dict[str, List]  # RAG ê²€ìƒ‰ ê²°ê³¼
    current_query: str  # í˜„ì¬ ê²€ìƒ‰ ì¿¼ë¦¬
    current_context: str  # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸


# ê³µí†µ ê²€ìƒ‰ í•¨ìˆ˜ë¥¼ ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ì½”ë“œ ì œê±°
def retrieve_info_for_role(
    state: DebateState, search_type: str, perspective: str
) -> DebateState:
    """
    ì—­í• ì— ë”°ë¥¸ ì •ë³´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ê³µí†µ í•¨ìˆ˜

    Args:
        state: í˜„ì¬ í† ë¡  ìƒíƒœ
        search_type: ê²€ìƒ‰ ìœ í˜• ("pro", "con", "judge")
        perspective: ê²€ìƒ‰ ê´€ì  ì„¤ëª… ("ì°¬ì„± ì¸¡ ê´€ì ", "ë°˜ëŒ€ ì¸¡ ê´€ì ", "í‰ê°€ ê¸°ì¤€ ê°ê´€ì  ì‚¬ì‹¤" ë“±)

    Returns:
        ì—…ë°ì´íŠ¸ëœ í† ë¡  ìƒíƒœ
    """
    # ê¸°ë³¸ ê²€ìƒ‰ ì¿¼ë¦¬ ì„¤ì •
    base_query = f"{state['topic']} {perspective}"

    # ê²€ìƒ‰ ìœ í˜•ë³„ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
    if search_type == "pro" and state["current_round"] > 1:
        # ì´ì „ ë°˜ëŒ€ ì¸¡ ì£¼ì¥ì´ ìˆìœ¼ë©´ ì°¸ê³ 
        prev_con_arguments = [m for m in state["messages"] if m["role"] == "ë°˜ëŒ€ ì¸¡"]
        if prev_con_arguments:
            last_con = prev_con_arguments[-1]["content"]
            base_query += f" {last_con}ì— ëŒ€í•œ ë°˜ë°•"

    elif search_type == "con" and state["messages"]:
        # ê°€ì¥ ìµœê·¼ ì°¬ì„± ì¸¡ ì£¼ì¥ ì°¸ê³ 
        pro_arguments = [e for e in state["messages"] if e["role"] == "ì°¬ì„± ì¸¡"]
        if pro_arguments:
            last_pro = pro_arguments[-1]["content"]
            base_query += f" {last_pro}ì— ëŒ€í•œ ë°˜ë°•"

    # ê²€ìƒ‰ ì‹¤í–‰
    context, docs = "", []
    if state["vector_store"]:
        # ê²€ìƒ‰ ìœ í˜•ì— ë”°ë¼ k ê°’ ì¡°ì •
        k = 2 if search_type == "judge" else 3
        context, docs = retrieve_relevant_info(base_query, state["vector_store"], k=k)

    # ìƒíƒœ ì—…ë°ì´íŠ¸
    new_state = state.copy()
    new_state["current_query"] = base_query
    new_state["current_context"] = context

    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì €ì¥
    if "retrieved_docs" not in new_state:
        new_state["retrieved_docs"] = {"pro": [], "con": []}

    # ê²€ìƒ‰ ìœ í˜•ì— ë”°ë¼ ì €ì¥ ìœ„ì¹˜ ê²°ì •
    if search_type in ["pro", "con"]:
        new_state["retrieved_docs"][search_type] = new_state["retrieved_docs"].get(
            search_type, []
        ) + ([doc.page_content for doc in docs] if docs else [])

    return new_state


# ì°¬ì„± ì¸¡ì„ ìœ„í•œ ê²€ìƒ‰ ë…¸ë“œ - ê³µí†µ í•¨ìˆ˜ í™œìš©
def retrieve_pro_info(state: DebateState) -> DebateState:
    """ì°¬ì„± ì¸¡ ì •ë³´ ê²€ìƒ‰ ë…¸ë“œ"""
    return retrieve_info_for_role(
        state, search_type="pro", perspective="ì°¬ì„± ì¥ì  ì´ìœ  ê·¼ê±°"
    )


# ë°˜ëŒ€ ì¸¡ì„ ìœ„í•œ ê²€ìƒ‰ ë…¸ë“œ - ê³µí†µ í•¨ìˆ˜ í™œìš©
def retrieve_con_info(state: DebateState) -> DebateState:
    """ë°˜ëŒ€ ì¸¡ ì •ë³´ ê²€ìƒ‰ ë…¸ë“œ"""
    return retrieve_info_for_role(
        state, search_type="con", perspective="ë°˜ëŒ€ ë‹¨ì  ë¬¸ì œì  ê·¼ê±°"
    )


# ì‹¬íŒì„ ìœ„í•œ ê²€ìƒ‰ ë…¸ë“œ - ê³µí†µ í•¨ìˆ˜ í™œìš©
def retrieve_judge_info(state: DebateState) -> DebateState:
    """ì‹¬íŒ ì •ë³´ ê²€ìƒ‰ ë…¸ë“œ"""
    return retrieve_info_for_role(
        state, search_type="judge", perspective="í‰ê°€ ê¸°ì¤€ ê°ê´€ì  ì‚¬ì‹¤"
    )


# ì°¬ì„± ì¸¡ ì—ì´ì „íŠ¸ ë…¸ë“œ - RAG ì œê±° ë° ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
def pro_agent(state: DebateState) -> DebateState:
    """ì°¬ì„± ì¸¡ ì—ì´ì „íŠ¸ í•¨ìˆ˜"""
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompt = "ë‹¹ì‹ ì€ ë…¼ë¦¬ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ì°¬ì„± ì¸¡ í† ë¡ ìì…ë‹ˆë‹¤."

    # ë©”ì‹œì§€ ì¤€ë¹„
    messages = [SystemMessage(content=system_prompt)]

    # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    for msg in state["messages"]:
        if msg["role"] == "assistant":
            messages.append(AIMessage(content=msg["content"]))
        else:
            messages.append(HumanMessage(content=f"{msg['role']}: {msg['content']}"))

    # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
    context = state.get("current_context", "")

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    if state["current_round"] == 1:
        prompt = f"""
        ë‹¹ì‹ ì€ '{state['topic']}'ì— ëŒ€í•´ ì°¬ì„± ì…ì¥ì„ ê°€ì§„ í† ë¡ ìì…ë‹ˆë‹¤.
        
        ë‹¤ìŒì€ ì´ ì£¼ì œì™€ ê´€ë ¨ëœ ì •ë³´ì…ë‹ˆë‹¤:
        {context}
        
        ë…¼ë¦¬ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ì°¬ì„± ì¸¡ ì£¼ì¥ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
        ê°€ëŠ¥í•œ ê²½ìš° ì œê³µëœ ì •ë³´ì—ì„œ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì¸ìš©í•˜ì„¸ìš”.
        2 ~ 3ë¬¸ë‹¨, ê° ë¬¸ë‹¨ì€ 100ìë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
    else:
        # ì´ì „ ë°œì–¸ìì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜´
        previous_messages = [m for m in state["messages"] if m["role"] == "ë°˜ëŒ€ ì¸¡"]
        if previous_messages:
            last_con_message = previous_messages[-1]["content"]
            prompt = f"""
            ë‹¹ì‹ ì€ '{state['topic']}'ì— ëŒ€í•´ ì°¬ì„± ì…ì¥ì„ ê°€ì§„ í† ë¡ ìì…ë‹ˆë‹¤.
            
            ë‹¤ìŒì€ ì´ ì£¼ì œì™€ ê´€ë ¨ëœ ì •ë³´ì…ë‹ˆë‹¤:
            {context}
            
            ë°˜ëŒ€ ì¸¡ì˜ ë‹¤ìŒ ì£¼ì¥ì— ëŒ€í•´ ë°˜ë°•í•˜ê³ , ì°¬ì„± ì…ì¥ì„ ë” ê°•í™”í•´ì£¼ì„¸ìš”:

            ë°˜ëŒ€ ì¸¡ ì£¼ì¥: "{last_con_message}"

            ê°€ëŠ¥í•œ ê²½ìš° ì œê³µëœ ì •ë³´ì—ì„œ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì¸ìš©í•˜ì„¸ìš”.
            2 ~ 3ë¬¸ë‹¨, ê° ë¬¸ë‹¨ì€ 100ìë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            """

    # LLMì— ìš”ì²­
    messages.append(HumanMessage(content=prompt))
    # response = llm.invoke(messages)
    # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
    # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ ìƒì„±
    with st.container(border=True):
        st.markdown("**ğŸ”µ ì°¬ì„± ì¸¡ ì˜ê²¬ ì‘ì„± ì¤‘...**")
        response_placeholder = st.empty()
        collected_chunks = []
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
        for chunk in llm.stream(messages):
            if chunk.content:
                collected_chunks.append(chunk.content)
                # ë§ˆí¬ë‹¤ìš´ ì„œì‹ì„ ì ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í‘œì‹œ
                formatted_text = f"""
                {' '.join(collected_chunks)}
                """
                response_placeholder.markdown(formatted_text)
    
    full_response = ''.join(collected_chunks)

    # ìƒíƒœ ì—…ë°ì´íŠ¸
    new_state = state.copy()
    # new_state["messages"].append({"role": "ì°¬ì„± ì¸¡", "content": response.content})
    new_state["messages"].append({"role": "ì°¬ì„± ì¸¡", "content": full_response})
    new_state["current_speaker"] = SpeakerRole.CON

    return new_state


# ë°˜ëŒ€ ì¸¡ ì—ì´ì „íŠ¸ ë…¸ë“œ - RAG ì œê±° ë° ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
def con_agent(state: DebateState) -> DebateState:
    """ë°˜ëŒ€ ì¸¡ ì—ì´ì „íŠ¸ í•¨ìˆ˜"""
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompt = "ë‹¹ì‹ ì€ ë…¼ë¦¬ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ë°˜ëŒ€ ì¸¡ í† ë¡ ìì…ë‹ˆë‹¤. ì°¬ì„± ì¸¡ ì£¼ì¥ì— ëŒ€í•´ ì ê·¹ì ìœ¼ë¡œ ë°˜ë°•í•˜ì„¸ìš”."

    # ë©”ì‹œì§€ ì¤€ë¹„
    messages = [SystemMessage(content=system_prompt)]

    # ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    for msg in state["messages"]:
        if msg["role"] == "assistant":
            messages.append(AIMessage(content=msg["content"]))
        else:
            messages.append(HumanMessage(content=f"{msg['role']}: {msg['content']}"))

    # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
    context = state.get("current_context", "")

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    # ì°¬ì„± ì¸¡ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ ê°€ì ¸ì˜´
    previous_messages = [m for m in state["messages"] if m["role"] == "ì°¬ì„± ì¸¡"]
    last_pro_message = previous_messages[-1]["content"]
    prompt = f"""
    ë‹¹ì‹ ì€ '{state['topic']}'ì— ëŒ€í•´ ë°˜ëŒ€ ì…ì¥ì„ ê°€ì§„ í† ë¡ ìì…ë‹ˆë‹¤.
    
    ë‹¤ìŒì€ ì´ ì£¼ì œì™€ ê´€ë ¨ëœ ì •ë³´ì…ë‹ˆë‹¤:
    {context}
    
    ì°¬ì„± ì¸¡ì˜ ë‹¤ìŒ ì£¼ì¥ì— ëŒ€í•´ ë°˜ë°•í•˜ê³ , ë°˜ëŒ€ ì…ì¥ì„ ì œì‹œí•´ì£¼ì„¸ìš”:

    ì°¬ì„± ì¸¡ ì£¼ì¥: "{last_pro_message}"

    ê°€ëŠ¥í•œ ê²½ìš° ì œê³µëœ ì •ë³´ì—ì„œ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì¸ìš©í•˜ì„¸ìš”.
    2 ~ 3ë¬¸ë‹¨, ê° ë¬¸ë‹¨ì€ 100ìë‚´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
    """

    # LLMì— ìš”ì²­
    messages.append(HumanMessage(content=prompt))
    # response = llm.invoke(messages)
    # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
    # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ ìƒì„±
    with st.container(border=True):
        st.markdown("**ğŸ”´ ë°˜ëŒ€ ì¸¡ ì˜ê²¬ ì‘ì„± ì¤‘...**")
        response_placeholder = st.empty()
        collected_chunks = []
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
        for chunk in llm.stream(messages):
            if chunk.content:
                collected_chunks.append(chunk.content)
                # ë§ˆí¬ë‹¤ìš´ ì„œì‹ì„ ì ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í‘œì‹œ
                formatted_text = f"""
                {' '.join(collected_chunks)}
                """
                response_placeholder.markdown(formatted_text)
    
    full_response = ''.join(collected_chunks)

    # ìƒíƒœ ì—…ë°ì´íŠ¸
    new_state = state.copy()
    # new_state["messages"].append({"role": "ë°˜ëŒ€ ì¸¡", "content": response.content})
    new_state["messages"].append({"role": "ë°˜ëŒ€ ì¸¡", "content": full_response})
    new_state["current_round"] += 1

    # ë‹¤ìŒ ë¼ìš´ë“œ ì—¬ë¶€ ê²°ì •
    if new_state["current_round"] <= new_state["max_rounds"]:
        new_state["current_speaker"] = SpeakerRole.PRO
    else:
        new_state["current_speaker"] = SpeakerRole.JUDGE

    return new_state


# ì‹¬íŒ ì—ì´ì „íŠ¸ ë…¸ë“œ - RAG ì œê±° ë° ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
def judge_agent(state: DebateState) -> DebateState:
    """ì‹¬íŒ ì—ì´ì „íŠ¸ í•¨ìˆ˜"""
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    system_prompt = "ë‹¹ì‹ ì€ ê³µì •í•˜ê³  ë…¼ë¦¬ì ì¸ í† ë¡  ì‹¬íŒì…ë‹ˆë‹¤. ì–‘ì¸¡ì˜ ì£¼ì¥ì„ ë©´ë°€íˆ ê²€í† í•˜ê³  ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”."

    # ë©”ì‹œì§€ ì¤€ë¹„
    messages = [SystemMessage(content=system_prompt)]

    # ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©
    context = state.get("current_context", "")

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
    ë‹¤ìŒì€ '{state['topic']}'ì— ëŒ€í•œ ì°¬ë°˜ í† ë¡ ì…ë‹ˆë‹¤. ê° ì¸¡ì˜ ì£¼ì¥ì„ ë¶„ì„í•˜ê³  í‰ê°€í•´ì£¼ì„¸ìš”.
    
    ë‹¤ìŒì€ ì´ ì£¼ì œì™€ ê´€ë ¨ëœ ê°ê´€ì ì¸ ì •ë³´ì…ë‹ˆë‹¤:
    {context}

    í† ë¡  ë‚´ìš©:
    """
    for msg in state["messages"]:
        prompt += f"\n\n{msg['role']}: {msg['content']}"

    prompt += """
    
    ìœ„ í† ë¡ ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ í¬í•¨í•˜ëŠ” ì‹¬ì‚¬ í‰ê°€ë¥¼ í•´ì£¼ì„¸ìš”:
    1. ì–‘ì¸¡ ì£¼ì¥ì˜ í•µì‹¬ ìš”ì•½
    2. ê° ì¸¡ì´ ì‚¬ìš©í•œ ì£¼ìš” ë…¼ë¦¬ì™€ ì¦ê±°ì˜ ê°•ì ê³¼ ì•½ì 
    3. ì „ì²´ í† ë¡ ì˜ ìŠ¹ìì™€ ê·¸ ì´ìœ 
    4. ì–‘ì¸¡ ëª¨ë‘ì—ê²Œ ê°œì„ ì  ì œì•ˆ
    
    ê°€ëŠ¥í•œ ê²½ìš° ì œê³µëœ ê°ê´€ì  ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ í‰ê°€í•´ì£¼ì„¸ìš”.
    """

    # LLMì— ìš”ì²­
    messages.append(HumanMessage(content=prompt))
    # response = llm.invoke(messages)
    # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
    # ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ ìƒì„±
    with st.container(border=True):
        st.markdown("**ğŸ§‘â€âš–ï¸ ìµœì¢… í‰ê°€ ì‘ì„± ì¤‘...**")
        response_placeholder = st.empty()
        collected_chunks = []
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
        for chunk in llm.stream(messages):
            if chunk.content:
                collected_chunks.append(chunk.content)
                # ë§ˆí¬ë‹¤ìš´ ì„œì‹ì„ ì ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ í‘œì‹œ
                formatted_text = f"""
                {' '.join(collected_chunks)}
                """
                response_placeholder.markdown(formatted_text)
    
    full_response = ''.join(collected_chunks)

    # ìƒíƒœ ì—…ë°ì´íŠ¸
    new_state = state.copy()
    # new_state["messages"].append({"role": "ì‹¬íŒ", "content": response.content})
    new_state["messages"].append({"role": "ì‹¬íŒ", "content": full_response})
    new_state["debate_status"] = DebateStatus.COMPLETED
    new_state["current_speaker"] = SpeakerRole.COMPLETED
    return new_state


# ë¼ìš°í„° í•¨ìˆ˜ ì—…ë°ì´íŠ¸: ê²€ìƒ‰ ë…¸ë“œì™€ ì—ì´ì „íŠ¸ ë…¸ë“œ ê°„ì˜ ë¼ìš°íŒ…
def router(
    state: DebateState,
) -> Literal[
    "retrieve_pro_info",
    "pro_agent",
    "retrieve_con_info",
    "con_agent",
    "retrieve_judge_info",
    "judge",
    "END",
]:
    if state["debate_status"] == DebateStatus.COMPLETED:
        return "END"

    current_speaker = state["current_speaker"]

    # í˜„ì¬ í™”ìì— ë”°ë¼ ê²€ìƒ‰ ë…¸ë“œë¡œ ë¨¼ì € ë¼ìš°íŒ…
    if current_speaker == SpeakerRole.PRO:
        return "retrieve_pro_info"
    elif current_speaker == SpeakerRole.CON:
        return "retrieve_con_info"
    elif current_speaker == SpeakerRole.JUDGE:
        return "retrieve_judge_info"
    elif current_speaker == SpeakerRole.COMPLETED:
        return "END"


# Pro ê²€ìƒ‰ í›„ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…
def pro_router(state: DebateState) -> Literal["pro_agent"]:
    return "pro_agent"


# Con ê²€ìƒ‰ í›„ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…
def con_router(state: DebateState) -> Literal["con_agent"]:
    return "con_agent"


# Judge ê²€ìƒ‰ í›„ ì—ì´ì „íŠ¸ë¡œ ë¼ìš°íŒ…
def judge_router(state: DebateState) -> Literal["judge"]:
    return "judge"


# LangGraph ì›Œí¬í”Œë¡œìš° ì •ì˜ - ì„œë¸Œê·¸ë˜í”„ í™œìš©
def create_debate_graph() -> StateGraph:
    # ë©”ì¸ ê·¸ë˜í”„ ìƒì„±
    workflow = StateGraph(DebateState)

    # ê²€ìƒ‰ ë…¸ë“œ ì¶”ê°€
    workflow.add_node("retrieve_pro_info", retrieve_pro_info)
    workflow.add_node("retrieve_con_info", retrieve_con_info)
    workflow.add_node("retrieve_judge_info", retrieve_judge_info)

    # ì—ì´ì „íŠ¸ ë…¸ë“œ ì¶”ê°€
    workflow.add_node("pro_agent", pro_agent)
    workflow.add_node("con_agent", con_agent)
    workflow.add_node("judge", judge_agent)

    # ë¼ìš°íŒ… ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("retrieve_pro_info")

    # ê²€ìƒ‰ ë…¸ë“œì—ì„œ ì—ì´ì „íŠ¸ ë…¸ë“œë¡œ ë¼ìš°íŒ…
    workflow.add_edge("retrieve_pro_info", "pro_agent")
    workflow.add_edge("retrieve_con_info", "con_agent")
    workflow.add_edge("retrieve_judge_info", "judge")

    # ì—ì´ì „íŠ¸ ë…¸ë“œ ì´í›„ ë¼ìš°íŒ…
    workflow.add_conditional_edges(
        "pro_agent",
        router,
        {
            "retrieve_pro_info": "retrieve_pro_info",
            "retrieve_con_info": "retrieve_con_info",
            "retrieve_judge_info": "retrieve_judge_info",
            "END": END,
        },
    )

    workflow.add_conditional_edges(
        "con_agent",
        router,
        {
            "retrieve_pro_info": "retrieve_pro_info",
            "retrieve_con_info": "retrieve_con_info",
            "retrieve_judge_info": "retrieve_judge_info",
            "END": END,
        },
    )

    workflow.add_conditional_edges(
        "judge",
        router,
        {
            "retrieve_pro_info": "retrieve_pro_info",
            "retrieve_con_info": "retrieve_con_info",
            "retrieve_judge_info": "retrieve_judge_info",
            "END": END,
        },
    )

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    return workflow.compile()

# db ì—†ë‹¤ë©´ ì´ˆê¸°í™”
init_db()

# Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œì§
# ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ ì´ˆê¸°í™”
if "debate_active" not in st.session_state:
    st.session_state.debate_active = False
    st.session_state.debate_messages = []
    st.session_state.vector_store = None
    st.session_state.retrieved_docs = {"pro": [], "con": []}
    # st.session_state.debate_history = []  # í† ë¡  ì´ë ¥ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
    # DBì—ì„œ í† ë¡  ì´ë ¥ ë¡œë“œ
    db_exists = os.path.exists("debate_history.db")
    if db_exists:
        st.session_state.debate_history = load_debate_history()

# ì‚¬ì´ë“œë°”: ì„¤ì •
with st.sidebar:
    # st.header("í† ë¡  ì„¤ì •")

    # # í† ë¡  ì£¼ì œ ì…ë ¥
    # debate_topic = st.text_input(
    #     "í† ë¡  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”:", "ì¸ê³µì§€ëŠ¥ì´ ì¸ê°„ì˜ ì¼ìë¦¬ë¥¼ ëŒ€ì²´í•´ì•¼ í•œë‹¤"
    # )
    # max_rounds = st.slider("í† ë¡  ë¼ìš´ë“œ ìˆ˜", min_value=1, max_value=5, value=1)

    # # RAG ê¸°ëŠ¥ í™œì„±í™” ì˜µì…˜
    # enable_rag = st.checkbox(
    #     "RAG í™œì„±í™”", value=True, help="ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ í† ë¡ ì— í™œìš©í•©ë‹ˆë‹¤."
    # )
    # show_sources = st.checkbox(
    #     "ì¶œì²˜ í‘œì‹œ", value=True, help="ê²€ìƒ‰ëœ ì •ë³´ì˜ ì¶œì²˜ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
    # )
    tab1, tab2 = st.tabs(["í† ë¡  ì„¤ì •", "í† ë¡  ì´ë ¥"])
    
    # í† ë¡  ì„¤ì • íƒ­
    with tab1:
        st.header("í† ë¡  ì„¤ì •")

        # í† ë¡  ì£¼ì œ ì…ë ¥
        debate_topic = st.text_input(
            "í† ë¡  ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”:", "ì¸ê³µì§€ëŠ¥ì´ ì¸ê°„ì˜ ì¼ìë¦¬ë¥¼ ëŒ€ì²´í•´ì•¼ í•œë‹¤"
        )
        max_rounds = st.slider("í† ë¡  ë¼ìš´ë“œ ìˆ˜", min_value=1, max_value=5, value=1)

        # RAG ê¸°ëŠ¥ í™œì„±í™” ì˜µì…˜
        enable_rag = st.checkbox(
            "RAG í™œì„±í™”", value=True, help="ì™¸ë¶€ ì§€ì‹ì„ ê²€ìƒ‰í•˜ì—¬ í† ë¡ ì— í™œìš©í•©ë‹ˆë‹¤."
        )
        show_sources = st.checkbox(
            "ì¶œì²˜ í‘œì‹œ", value=True, help="ê²€ìƒ‰ëœ ì •ë³´ì˜ ì¶œì²˜ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."
        )

    # í† ë¡  ì´ë ¥ íƒ­
    with tab2:
        st.header("ì´ì „ í† ë¡  ì´ë ¥")
        
        # ë²„íŠ¼ì„ ë‚˜ë€íˆ ë°°ì¹˜í•˜ê¸° ìœ„í•œ ì»¬ëŸ¼ ìƒì„±
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ”„ ì´ë ¥ ìƒˆë¡œê³ ì¹¨"):
                st.rerun()
        with col2:
            if st.button("ğŸ—‘ï¸ ì „ì²´ ì´ë ¥ ì‚­ì œ"):
                if st.session_state.debate_history:
                    st.session_state.debate_history = []
                    st.success("ëª¨ë“  í† ë¡  ì´ë ¥ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.rerun()

        if not st.session_state.debate_history:
            st.info("ì €ì¥ëœ í† ë¡  ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ìµœì‹  í† ë¡ ì´ ìœ„ë¡œ ì˜¤ë„ë¡ ë¦¬ìŠ¤íŠ¸ ì—­ìˆœ ì •ë ¬
            for idx, history in enumerate(reversed(st.session_state.debate_history)):
                with st.container(border=True):
                    # ì œëª©ê³¼ ë²„íŠ¼ì„ ë‚˜ë€íˆ ë°°ì¹˜
                    col1, col2, col3 = st.columns([6, 1, 1])
                    with col1:
                        st.markdown(f"**{history['topic']}**")
                    with col2:
                        if st.button("ë³´ê¸°", key=f"view_{idx}"):
                            st.session_state.debate_messages = history['messages']
                            st.session_state.debate_active = True
                            st.rerun()
                    with col3:
                        if st.button("ì‚­ì œ", key=f"delete_{idx}"):
                            # ì‹¤ì œ ì¸ë±ìŠ¤ëŠ” ì—­ìˆœì´ë¯€ë¡œ len-1-idxë¡œ ê³„ì‚°
                            real_idx = len(st.session_state.debate_history) - 1 - idx
                            st.session_state.debate_history.pop(real_idx)
                            st.success("ì„ íƒí•œ í† ë¡  ì´ë ¥ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                            st.rerun()
                    
                    # ë‚ ì§œì™€ ë¼ìš´ë“œ ì •ë³´
                    st.caption(f"ë‚ ì§œ: {history['timestamp']}")
                    st.caption(f"ë¼ìš´ë“œ: {history['rounds']}")

# í† ë¡  ì‹œì‘ ë²„íŠ¼
if not st.session_state.debate_active and st.button("í† ë¡  ì‹œì‘"):
    st.session_state.vector_store = None
    st.session_state.retrieved_docs = {"pro": [], "con": []}

    # Vector Store ìƒì„± (RAG í™œì„±í™”ëœ ê²½ìš°)
    if enable_rag:
        with st.spinner("ì™¸ë¶€ ì§€ì‹ì„ ìˆ˜ì§‘í•˜ê³  ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            vector_store = create_vector_store(debate_topic)
            st.session_state.vector_store = vector_store

            if vector_store:
                st.success("ì™¸ë¶€ ì§€ì‹ ê²€ìƒ‰ ì¤€ë¹„ ì™„ë£Œ!")
            else:
                st.warning(
                    "ì™¸ë¶€ ì§€ì‹ ê²€ìƒ‰ì„ ìœ„í•œ ì¤€ë¹„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ í† ë¡ ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
                )

    # ê·¸ë˜í”„ ìƒì„±
    debate_graph = create_debate_graph()

    # ì´ˆê¸° ìƒíƒœ ì„¤ì • ì—…ë°ì´íŠ¸
    initial_state: DebateState = {
        "topic": debate_topic,
        "messages": [],
        "current_round": 1,
        "max_rounds": max_rounds,
        "current_speaker": SpeakerRole.PRO,
        "debate_status": DebateStatus.ACTIVE,
        "vector_store": st.session_state.vector_store,
        "retrieved_docs": {"pro": [], "con": []},
        "current_query": "",
        "current_context": "",
    }

    # í† ë¡  ì‹œì‘
    with st.spinner("í† ë¡ ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤... ì™„ë£Œê¹Œì§€ ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
        # ê·¸ë˜í”„ ì‹¤í–‰ - stream ëŒ€ì‹  invoke ì‚¬ìš©
        result = debate_graph.invoke(initial_state)

        # ê²°ê³¼ë¥¼ ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ì €ì¥
        st.session_state.debate_messages = result["messages"]
        st.session_state.debate_active = True
        st.session_state.retrieved_docs = result.get(
            "retrieved_docs", {"pro": [], "con": []}
        )

        # í† ë¡  ì™„ë£Œ í›„ ì´ë ¥ì„ dbì— ì €ì¥
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
        
        # DBì— ì €ì¥
        conn = sqlite3.connect("debate_history.db")
        c = conn.cursor()
        c.execute("""
            INSERT INTO debates (topic, date, rounds, messages)
            VALUES (?, ?, ?, ?)
        """, (debate_topic, current_time, max_rounds, json.dumps(result["messages"])))
        conn.commit()
        conn.close()
        
        # ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ ì—…ë°ì´íŠ¸
        db_exists = os.path.exists("debate_history.db")
        if db_exists:
            st.session_state.debate_history = load_debate_history()

    # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ê²°ê³¼ í‘œì‹œ
    st.rerun()

# í† ë¡  ë‚´ìš© í‘œì‹œ
if st.session_state.debate_active:
    # í† ë¡  ì£¼ì œ í‘œì‹œ
    st.header(f"í† ë¡  ì£¼ì œ: {debate_topic}")

    # í† ë¡  ë‚´ìš© í‘œì‹œ
    st.header("í† ë¡  ì§„í–‰ ìƒí™©")

    messages = st.session_state.debate_messages
    total_rounds = len([m for m in messages if m["role"] == "ì°¬ì„± ì¸¡"])

    # ë¼ìš´ë“œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‘œì‹œ
    for round_num in range(1, total_rounds + 1):
        st.subheader(f"ë¼ìš´ë“œ {round_num}")

        # ì´ ë¼ìš´ë“œì˜ ì°¬ì„±ì¸¡ ë©”ì‹œì§€ ì°¾ê¸° (ì¸ë±ìŠ¤ëŠ” (ë¼ìš´ë“œ-1)*2)
        pro_index = (round_num - 1) * 2
        if pro_index < len(messages) and messages[pro_index]["role"] == "ì°¬ì„± ì¸¡":
            with st.container(border=True):
                st.markdown("**ğŸ”µ ì°¬ì„± ì¸¡:**")
                st.write(messages[pro_index]["content"])

        # ì´ ë¼ìš´ë“œì˜ ë°˜ëŒ€ì¸¡ ë©”ì‹œì§€ ì°¾ê¸° (ì¸ë±ìŠ¤ëŠ” (ë¼ìš´ë“œ-1)*2 + 1)
        con_index = (round_num - 1) * 2 + 1
        if con_index < len(messages) and messages[con_index]["role"] == "ë°˜ëŒ€ ì¸¡":
            with st.container(border=True):
                st.markdown("**ğŸ”´ ë°˜ëŒ€ ì¸¡:**")
                st.write(messages[con_index]["content"])

        st.divider()

    # ì‹¬íŒ í‰ê°€ í‘œì‹œ (ë§ˆì§€ë§‰ ë©”ì‹œì§€)
    if messages and messages[-1]["role"] == "ì‹¬íŒ":
        with st.container(border=True):
            st.subheader("ğŸ§‘â€âš–ï¸ ìµœì¢… í‰ê°€")
            st.write(messages[-1]["content"])

    # ê²€ìƒ‰ëœ ì¶œì²˜ ì •ë³´ í‘œì‹œ (ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°)
    if (
        show_sources
        and st.session_state.retrieved_docs
        and (
            st.session_state.retrieved_docs.get("pro")
            or st.session_state.retrieved_docs.get("con")
        )
    ):
        with st.expander("ì‚¬ìš©ëœ ì°¸ê³  ìë£Œ ë³´ê¸°"):
            st.subheader("ì°¬ì„± ì¸¡ ì°¸ê³  ìë£Œ")
            for i, doc in enumerate(
                st.session_state.retrieved_docs.get("pro", [])[:3]
            ):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                st.markdown(f"**ì¶œì²˜ {i+1}**")
                st.text(doc[:300] + "..." if len(doc) > 300 else doc)
                st.divider()

            st.subheader("ë°˜ëŒ€ ì¸¡ ì°¸ê³  ìë£Œ")
            for i, doc in enumerate(
                st.session_state.retrieved_docs.get("con", [])[:3]
            ):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                st.markdown(f"**ì¶œì²˜ {i+1}**")
                st.text(doc[:300] + "..." if len(doc) > 300 else doc)
                st.divider()

    # ë‹¤ì‹œ ì‹œì‘ ë²„íŠ¼
    if st.button("ìƒˆ í† ë¡  ì‹œì‘"):
        st.session_state.debate_active = False
        st.session_state.debate_messages = []
        st.session_state.vector_store = None
        st.session_state.retrieved_docs = {"pro": [], "con": []}
        st.rerun()
